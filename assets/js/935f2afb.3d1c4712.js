"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[53],{1109:e=>{e.exports=JSON.parse('{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"tutorialSidebar":[{"type":"link","label":"Azure OpenAI Proxy Service","href":"/azure-openai-service-proxy/","docId":"Introduction"},{"type":"category","label":"Service installation","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"OpenAI proxy service","href":"/azure-openai-service-proxy/service-installation/OpenAI-Proxy","docId":"service-installation/OpenAI-Proxy"},{"type":"link","label":"Scaling the Proxy Service","href":"/azure-openai-service-proxy/service-installation/scaling-proxy-service","docId":"service-installation/scaling-proxy-service"},{"type":"category","label":"Testing","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Proxy service","href":"/azure-openai-service-proxy/service-installation/testing/testing","docId":"service-installation/testing/testing"},{"type":"link","label":"Load testing","href":"/azure-openai-service-proxy/service-installation/testing/load-testing","docId":"service-installation/testing/load-testing"}],"href":"/azure-openai-service-proxy/category/testing"}],"href":"/azure-openai-service-proxy/category/service-installation"},{"type":"link","label":"Playground installation","href":"/azure-openai-service-proxy/playground-installation","docId":"playground-installation"},{"type":"category","label":"Authorisation","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Event authorization","href":"/azure-openai-service-proxy/Authorization/authorisation","docId":"Authorization/authorisation"},{"type":"link","label":"Azure OpenAI rate limits","href":"/azure-openai-service-proxy/Authorization/openai-rate-limits","docId":"Authorization/openai-rate-limits"},{"type":"link","label":"Adding an event code","href":"/azure-openai-service-proxy/Authorization/adding-event","docId":"Authorization/adding-event"}],"href":"/azure-openai-service-proxy/category/authorisation"},{"type":"link","label":"OpenAI model deployments","href":"/azure-openai-service-proxy/openai-deployments","docId":"openai-deployments"},{"type":"link","label":"Using the Playground","href":"/azure-openai-service-proxy/playground","docId":"playground"},{"type":"category","label":"Developer endpoints","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Endpoint access","href":"/azure-openai-service-proxy/raw-api-access/introduction","docId":"raw-api-access/introduction"},{"type":"link","label":"Chat completions API","href":"/azure-openai-service-proxy/raw-api-access/chat-completion","docId":"raw-api-access/chat-completion"},{"type":"link","label":"Embeddings API","href":"/azure-openai-service-proxy/raw-api-access/embedding","docId":"raw-api-access/embedding"}],"href":"/azure-openai-service-proxy/category/developer-endpoints"}]},"docs":{"Authorization/adding-event":{"id":"Authorization/adding-event","title":"Adding an event code","description":"For now, you add an event via the Azure Storage Account Storage browser. The Storage browser is available in the Azure Portal, under the Storage account resource.","sidebar":"tutorialSidebar"},"Authorization/authorisation":{"id":"Authorization/authorisation","title":"Event authorization","description":"Access to the proxy service endpoint is controlled by an event code. The proxy service is accessible when the current UTC is between the StartUTC and the EndUTC times and the event is active. The event code is passed in the openai-event-code header. If the event code is not passed, or the event code is not active, or the current UTC is not between the StartUTC and the EndUTC times, the proxy service will return a 401 unauthorized error.","sidebar":"tutorialSidebar"},"Authorization/openai-rate-limits":{"id":"Authorization/openai-rate-limits","title":"Azure OpenAI rate limits","description":"Azure OpenAI model deployments have two limits, the first being tokens per minute, and the second being requests per minute. You are most likely to hit the Tokens per minute limit especially as you scale up the number of users using the system.","sidebar":"tutorialSidebar"},"Introduction":{"id":"Introduction","title":"Azure OpenAI Proxy Service","description":"<Social","sidebar":"tutorialSidebar"},"openai-deployments":{"id":"openai-deployments","title":"OpenAI model deployments","description":"From the Azure Portal, select the Azure OpenAI resource, then select the Deployments tab, and finally select Create deployment. Enter a friendly name for the deployment, and select the model and the capacity. The capacity is the number of requests per minute. The capacity can be changed at any time. The deployment will take a few minutes to provision.","sidebar":"tutorialSidebar"},"playground":{"id":"playground","title":"Using the Playground","description":"The Playground is a web-based application that allows users to experiment with OpenAI Chat Completions.","sidebar":"tutorialSidebar"},"playground-installation":{"id":"playground-installation","title":"Playground installation","description":"to be completed","sidebar":"tutorialSidebar"},"raw-api-access/chat-completion":{"id":"raw-api-access/chat-completion","title":"Chat completions API","description":"The OpenAI proxy service chat completion endpoint is a REST API that generates a response to a user\'s prompt. Requests are forwarded to the Azure OpenAI service and the response is returned to the caller.","sidebar":"tutorialSidebar"},"raw-api-access/embedding":{"id":"raw-api-access/embedding","title":"Embeddings API","description":"The OpenAI proxy service also supports the OpenAI Embeddings API. The Embeddings API is a REST API that generates embeddings for a given text. Requests are forwarded to the Azure OpenAI service and the response is returned to the caller.","sidebar":"tutorialSidebar"},"raw-api-access/introduction":{"id":"raw-api-access/introduction","title":"Endpoint access","description":"The Azure OpenAI proxy service provides access to the Azure OpenAI APIs for developers to build applications, again using a time bound event code. Initially, there are two REST endpoints available via the proxy service, chat completions, and embeddings.","sidebar":"tutorialSidebar"},"service-installation/OpenAI-Proxy":{"id":"service-installation/OpenAI-Proxy","title":"OpenAI proxy service","description":"The solution consists of two parts; the proxy service and a web client with a similar look and feel to the official Azure OpenAI Playground. The proxy service is a Python FastAPI app that proxies requests to the OpenAI API.","sidebar":"tutorialSidebar"},"service-installation/scaling-proxy-service":{"id":"service-installation/scaling-proxy-service","title":"Scaling the Proxy Service","description":"The proxy service is stateless and scales vertically and horizontally. By default, the proxy service is configured to scale up to 10 replicas. The proxy service is configured to scale up to 10 replicas. The number of replicas can be changed from the Azure Portal or from the az cli. For example, to scale to 30 replicas using the az cli, change the:","sidebar":"tutorialSidebar"},"service-installation/testing/load-testing":{"id":"service-installation/testing/load-testing","title":"Load testing","description":"There are several load testing tools available. The recommended tool is JMeter as the test plan can be deployed to Azure. The JMeter test plan is located in the loadtest folder. The test plan is configured to run 100 concurrent users, generating 4 requests per minute.","sidebar":"tutorialSidebar"},"service-installation/testing/testing":{"id":"service-installation/testing/testing","title":"Proxy service","description":"There are various options to test the endpoint. The simplest is to use Curl from either PowerShell or a Bash/zsh terminal. For example:","sidebar":"tutorialSidebar"}}}')}}]);