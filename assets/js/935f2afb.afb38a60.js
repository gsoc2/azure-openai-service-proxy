"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[53],{1109:e=>{e.exports=JSON.parse('{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"tutorialSidebar":[{"type":"link","label":"Azure OpenAI Proxy Service","href":"/azure-openai-service-proxy/","docId":"Introduction"},{"type":"category","label":"Service installation","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"OpenAI proxy service","href":"/azure-openai-service-proxy/service-installation/OpenAI-Proxy","docId":"service-installation/OpenAI-Proxy"},{"type":"link","label":"Scaling the Proxy Service","href":"/azure-openai-service-proxy/service-installation/scaling-proxy-service","docId":"service-installation/scaling-proxy-service"},{"type":"link","label":"Managing models","href":"/azure-openai-service-proxy/service-installation/openai-deployments","docId":"service-installation/openai-deployments"},{"type":"category","label":"Testing","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Proxy service","href":"/azure-openai-service-proxy/service-installation/testing/testing","docId":"service-installation/testing/testing"},{"type":"link","label":"Load testing","href":"/azure-openai-service-proxy/service-installation/testing/load-testing","docId":"service-installation/testing/load-testing"}],"href":"/azure-openai-service-proxy/category/testing"}],"href":"/azure-openai-service-proxy/category/service-installation"},{"type":"category","label":"Proxy Management","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Managing models","href":"/azure-openai-service-proxy/Configuration/model-deployments","docId":"Configuration/model-deployments"},{"type":"link","label":"Managing Rate limits","href":"/azure-openai-service-proxy/Configuration/openai-rate-limits","docId":"Configuration/openai-rate-limits"},{"type":"link","label":"Managing Events","href":"/azure-openai-service-proxy/Configuration/events","docId":"Configuration/events"}],"href":"/azure-openai-service-proxy/category/proxy-management"},{"type":"link","label":"The Playground","href":"/azure-openai-service-proxy/playground","docId":"playground"},{"type":"category","label":"Developer endpoints","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Endpoint access","href":"/azure-openai-service-proxy/raw-api-access/introduction","docId":"raw-api-access/introduction"},{"type":"link","label":"Chat completions API","href":"/azure-openai-service-proxy/raw-api-access/chat-completion","docId":"raw-api-access/chat-completion"},{"type":"link","label":"Completions API","href":"/azure-openai-service-proxy/raw-api-access/completions","docId":"raw-api-access/completions"},{"type":"link","label":"Embeddings API","href":"/azure-openai-service-proxy/raw-api-access/embedding","docId":"raw-api-access/embedding"},{"type":"link","label":"Image Generation API","href":"/azure-openai-service-proxy/raw-api-access/images-generations","docId":"raw-api-access/images-generations"}],"href":"/azure-openai-service-proxy/category/developer-endpoints"}]},"docs":{"Configuration/events":{"id":"Configuration/events","title":"Managing Events","description":"The OpenAI Proxy Service supports multiple events. Each event has a unique EventCode. The EventCode is used to identify the event when calling the proxy service.","sidebar":"tutorialSidebar"},"Configuration/model-deployments":{"id":"Configuration/model-deployments","title":"Managing models","description":"The Management API","sidebar":"tutorialSidebar"},"Configuration/openai-rate-limits":{"id":"Configuration/openai-rate-limits","title":"Managing Rate limits","description":"Azure OpenAI model deployments have two limits, the first being tokens per minute, and the second being requests per minute. You are most likely to hit the Tokens per minute limit especially as you scale up the number of users using the system.","sidebar":"tutorialSidebar"},"Introduction":{"id":"Introduction","title":"Azure OpenAI Proxy Service","description":"<Social","sidebar":"tutorialSidebar"},"playground":{"id":"playground","title":"The Playground","description":"The Azure OpenAI proxy service provides a Playground like experience for developers to explore the Azure OpenAI chat completion using the time bound event code. The Playground is UX modeled on the official Azure OpenAI Playground, so if you\'ve used the official Playground, you\'ll be familiar with the Playground.","sidebar":"tutorialSidebar"},"raw-api-access/chat-completion":{"id":"raw-api-access/chat-completion","title":"Chat completions API","description":"The OpenAI proxy service chat completion endpoint is a REST API that generates a response to a messages. Requests are forwarded to the Azure OpenAI service and the response is returned to the caller.","sidebar":"tutorialSidebar"},"raw-api-access/completions":{"id":"raw-api-access/completions","title":"Completions API","description":"The OpenAI proxy service completion endpoint is a REST API that generates a response to a prompts. Requests are forwarded to the Azure OpenAI service and the response is returned to the caller.","sidebar":"tutorialSidebar"},"raw-api-access/embedding":{"id":"raw-api-access/embedding","title":"Embeddings API","description":"The OpenAI proxy service also supports the OpenAI Embeddings API. The Embeddings API is a REST API that generates embeddings for a given text. Requests are forwarded to the Azure OpenAI service and the response is returned to the caller.","sidebar":"tutorialSidebar"},"raw-api-access/images-generations":{"id":"raw-api-access/images-generations","title":"Image Generation API","description":"The Azure OpenAI proxy service supports the Azure OpenAI Image Generation API, as of November 2023, the OpenAI Dall-e 2 model is supported. Requests are forwarded to the Azure OpenAI service and the response is returned to the caller.","sidebar":"tutorialSidebar"},"raw-api-access/introduction":{"id":"raw-api-access/introduction","title":"Endpoint access","description":"The Azure OpenAI proxy service provides access to the Azure OpenAI APIs for developers to build applications, again using a time bound event code. Initially, the following APIs are available via the proxy service:","sidebar":"tutorialSidebar"},"service-installation/openai-deployments":{"id":"service-installation/openai-deployments","title":"Managing models","description":"Understanding Azure OpenAI model deployments","sidebar":"tutorialSidebar"},"service-installation/OpenAI-Proxy":{"id":"service-installation/OpenAI-Proxy","title":"OpenAI proxy service","description":"The solution consists of two parts; the proxy service and a web client with a similar look and feel to the official Azure OpenAI Playground. The proxy service is a Python FastAPI app that proxies requests to the OpenAI API.","sidebar":"tutorialSidebar"},"service-installation/scaling-proxy-service":{"id":"service-installation/scaling-proxy-service","title":"Scaling the Proxy Service","description":"The proxy service is stateless and scales vertically and horizontally. By default, the proxy service is configured to scale up to 10 replicas. The proxy service is configured to scale up to 10 replicas. The number of replicas can be changed from the Azure Portal or from the az cli. For example, to scale to 30 replicas using the az cli, change the:","sidebar":"tutorialSidebar"},"service-installation/testing/load-testing":{"id":"service-installation/testing/load-testing","title":"Load testing","description":"There are several load testing tools available. The recommended tool is JMeter as the test plan can be deployed to Azure. The JMeter test plan is located in the loadtest folder. The test plan is configured to run 100 concurrent users, generating 4 requests per minute.","sidebar":"tutorialSidebar"},"service-installation/testing/testing":{"id":"service-installation/testing/testing","title":"Proxy service","description":"There are various options to test the endpoint. The simplest is to use Curl from either PowerShell or a Bash/zsh terminal. For example:","sidebar":"tutorialSidebar"}}}')}}]);